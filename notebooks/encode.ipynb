{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from tqdm import trange\n",
    "from utils.video import read_video, transform_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = read_video(\"../examples/sample_video_ecamera.hevc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array([transform_img(x) for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 15:59:05.017313715 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-08-30 15:59:05.017329214 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "# load model session\n",
    "options = ort.SessionOptions()\n",
    "provider = 'CUDAExecutionProvider'\n",
    "session = ort.InferenceSession('../gpt2m/encoder.onnx', options, [provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shapes :  {'big_img': ['b', 3, 128, 256]}\n",
      "output shapes:  {'encoding_indices': ['b', 8, 16]}\n"
     ]
    }
   ],
   "source": [
    "# print shapes\n",
    "input_shapes = {i.name: i.shape for i in session.get_inputs()}\n",
    "output_shapes = {i.name: i.shape for i in session.get_outputs()}\n",
    "print('input shapes : ', input_shapes)\n",
    "print('output shapes: ', output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [00:10<00:00, 112.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# encoding loop\n",
    "tokens = []\n",
    "for i in trange(len(frames)):\n",
    "  outputs = session.run(None, {'big_img': frames[i].transpose(2,0,1)[None].astype(np.float32)})\n",
    "  outputs = {o.name: x for o,x in zip(session.get_outputs(), outputs)}\n",
    "  tokens.append(outputs['encoding_indices'].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokens! now head over to the decoding notebook\n",
    "tokens = np.array(tokens)\n",
    "np.save(\"../examples/tokens.npy\", tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
